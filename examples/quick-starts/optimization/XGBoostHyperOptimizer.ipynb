{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# `optimization.XGBoostHyperOptimizer`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import slickml\n",
    "\n",
    "print(f\"Loaded SlickML Version = {slickml.__version__}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded SlickML Version = 0.2.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from slickml.optimization import XGBoostHyperOptimizer\n",
    "\n",
    "help(XGBoostHyperOptimizer)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Help on class XGBoostHyperOptimizer in module slickml.optimization._hyperopt:\n",
      "\n",
      "class XGBoostHyperOptimizer(slickml.base._estimator.BaseXGBoostEstimator)\n",
      " |  XGBoostHyperOptimizer(num_boost_round: Optional[int] = 200, sparse_matrix: Optional[bool] = False, scale_mean: Optional[bool] = False, scale_std: Optional[bool] = False, importance_type: Optional[str] = 'total_gain', params: Optional[Dict[str, Union[str, float, int]]] = None, n_iter: Optional[int] = 100, n_splits: Optional[int] = 4, metrics: Optional[str] = 'auc', objective: Optional[str] = 'binary:logistic', params_bounds: Optional[Dict[str, Any]] = None, early_stopping_rounds: Optional[int] = 20, stratified: Optional[bool] = True, shuffle: Optional[bool] = True, random_state: Optional[int] = 1367, verbose: Optional[bool] = True) -> None\n",
      " |  \n",
      " |  XGBoost Hyper-Parameters Tuner using HyperOpt Optimization.\n",
      " |  \n",
      " |  This is wrapper using HyperOpt [hyperopt]_ a Python library for serial and parallel optimization\n",
      " |  over search spaces, which may include real-valued, discrete, and conditional dimensions to tune the\n",
      " |  hyper-parameter of XGBoost [xgboost-api]_ using ``xgboost.cv()`` functionality with n-folds\n",
      " |  cross-validation iteratively. This feature can be used to find the set of optimized set of\n",
      " |  hyper-parameters for both classification and regression tasks.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The optimizier objective is always to minimize the target values. Therefore, in case of using a\n",
      " |  metric such as ``auc``, or ``aucpr`` the negative value of the metric will be minimized.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_iter : int, optional\n",
      " |      Maximum number of iteration rounds for hyper-parameters tuning before convergance, by default 100\n",
      " |  \n",
      " |  n_splits : int, optional\n",
      " |      Number of folds for cross-validation, by default 4\n",
      " |  \n",
      " |  metrics : str, optional\n",
      " |      Metrics to be tracked at cross-validation fitting time depends on the task\n",
      " |      (classification vs regression) with possible values of \"auc\", \"aucpr\", \"error\", \"logloss\",\n",
      " |      \"rmse\", \"rmsle\", \"mae\". Note this is different than `eval_metric` that needs to be passed to\n",
      " |      `params` dict, by default \"auc\"\n",
      " |  \n",
      " |  objective : str, optional\n",
      " |      Objective function depending on the task whether it is regression or classification. Possible\n",
      " |      objectives for classification ``\"binary:logistic\"`` and for regression ``\"reg:logistic\"``,\n",
      " |      ``\"reg:squarederror\"``, and ``\"reg:squaredlogerror\"``, by default \"binary:logistic\"\n",
      " |  \n",
      " |  params_bounds : Dict[str, Any], optional\n",
      " |      Set of hyper-parameters boundaries for HyperOpt using``hyperopt.hp`` and `hyperopt.pyll_utils`,\n",
      " |      by default {\"max_depth\" : (2, 7), \"learning_rate\" : (0, 1), \"min_child_weight\" : (1, 20),\n",
      " |      \"colsample_bytree\": (0.1, 1.0), \"subsample\" : (0.1, 1), \"gamma\" : (0, 1),\n",
      " |      \"reg_alpha\" : (0, 1), \"reg_lambda\" : (0, 1)}\n",
      " |  \n",
      " |  num_boost_round : int, optional\n",
      " |      Number of boosting rounds to fit a model, by default 200\n",
      " |  \n",
      " |  early_stopping_rounds : int, optional\n",
      " |      The criterion to early abort the ``xgboost.cv()`` phase if the test metric is not improved,\n",
      " |      by default 20\n",
      " |  \n",
      " |  random_state : int, optional\n",
      " |      Random seed number, by default 1367\n",
      " |  \n",
      " |  stratified : bool, optional\n",
      " |      Whether to use stratificaiton of the targets (only available for classification tasks) to run\n",
      " |      ``xgboost.cv()`` to find the best number of boosting round at each fold of each iteration,\n",
      " |      by default True\n",
      " |  \n",
      " |  shuffle : bool, optional\n",
      " |      Whether to shuffle data to have the ability of building stratified folds in ``xgboost.cv()``,\n",
      " |      by default True\n",
      " |  \n",
      " |  sparse_matrix : bool, optional\n",
      " |      Whether to convert the input features to sparse matrix with csr format or not. This would\n",
      " |      increase the speed of feature selection for relatively large/sparse datasets. Consequently,\n",
      " |      this would actually act like an un-optimize solution for dense feature matrix. Additionally,\n",
      " |      this parameter cannot be used along with ``scale_mean=True`` standardizing the feature matrix\n",
      " |      to have a mean value of zeros would turn the feature matrix into a dense matrix. Therefore,\n",
      " |      by default our API banned this feature, by default False\n",
      " |  \n",
      " |  scale_mean : bool, optional\n",
      " |      Whether to standarize the feauture matrix to have a mean value of zero per feature (center\n",
      " |      the features before scaling). As laid out in ``sparse_matrix``, ``scale_mean=False`` when\n",
      " |      using ``sparse_matrix=True``, since centering the feature matrix would decrease the sparsity\n",
      " |      and in practice it does not make any sense to use sparse matrix method and it would make\n",
      " |      it worse. The ``StandardScaler`` object can be accessed via ``cls.scaler_`` if ``scale_mean`` or\n",
      " |      ``scale_strd`` is used unless it is ``None``, by default False\n",
      " |  \n",
      " |  scale_std : bool, optional\n",
      " |      Whether to scale the feauture matrix to have unit variance (or equivalently, unit standard\n",
      " |      deviation) per feature. The ``StandardScaler`` object can be accessed via ``cls.scaler_``\n",
      " |      if ``scale_mean`` or ``scale_strd`` is used unless it is ``None``, by default False\n",
      " |  \n",
      " |  importance_type : str, optional\n",
      " |      Importance type of ``xgboost.train()`` with possible values ``\"weight\"``, ``\"gain\"``,\n",
      " |      ``\"total_gain\"``, ``\"cover\"``, ``\"total_cover\"``, by default \"total_gain\"\n",
      " |  \n",
      " |  verbose : bool, optional\n",
      " |      Whether to show the HyperOpt Optimization progress at each iteration, by default True\n",
      " |  \n",
      " |  Methods\n",
      " |  -------\n",
      " |  fit(X, y)\n",
      " |      Fits the HyperOpt optimization algorithm to tune the hyper-parameters\n",
      " |  \n",
      " |  get_best_params()\n",
      " |      Returns the tuned hyper-parameters as a dictionary\n",
      " |  \n",
      " |  get_results()\n",
      " |      Returns all the optimization trials\n",
      " |  \n",
      " |  get_trials()\n",
      " |      Return the trials object\n",
      " |  \n",
      " |  get_params_bounds()\n",
      " |      Returns the parameters boundaries\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  best_params_\n",
      " |      Returns the tuned hyper-parameters as a dictionary\n",
      " |  \n",
      " |  results_\n",
      " |      Returns all the optimization trials as results\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  .. [xgboost-api] https://xgboost.readthedocs.io/en/latest/python/python_api.html\n",
      " |  .. [hyperopt] https://github.com/hyperopt/hyperopt\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      XGBoostHyperOptimizer\n",
      " |      slickml.base._estimator.BaseXGBoostEstimator\n",
      " |      abc.ABC\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |  \n",
      " |  __init__(self, num_boost_round: Optional[int] = 200, sparse_matrix: Optional[bool] = False, scale_mean: Optional[bool] = False, scale_std: Optional[bool] = False, importance_type: Optional[str] = 'total_gain', params: Optional[Dict[str, Union[str, float, int]]] = None, n_iter: Optional[int] = 100, n_splits: Optional[int] = 4, metrics: Optional[str] = 'auc', objective: Optional[str] = 'binary:logistic', params_bounds: Optional[Dict[str, Any]] = None, early_stopping_rounds: Optional[int] = 20, stratified: Optional[bool] = True, shuffle: Optional[bool] = True, random_state: Optional[int] = 1367, verbose: Optional[bool] = True) -> None\n",
      " |  \n",
      " |  __post_init__(self) -> None\n",
      " |      Post instantiation validations and assignments.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |  \n",
      " |  fit(self, X: Union[pandas.core.frame.DataFrame, numpy.ndarray], y: Union[List[float], numpy.ndarray, pandas.core.series.Series]) -> None\n",
      " |      Fits the main hyper-parameter tuning algorithm.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      At each iteration, one set of parameters gets passed from the `params_bounds` and the\n",
      " |      evaluation occurs based on the cross-validation results. Hyper optimizier always\n",
      " |      minimizes the objectives. Therefore, based on the `metrics` we should be careful\n",
      " |      when using `self.metrics` that are supposed to get maximized i.e. `auc`. For those,\n",
      " |      we can maximize `(-1) * metric`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : Union[pd.DataFrame, np.ndarray]\n",
      " |          Input data for training (features)\n",
      " |      \n",
      " |      y : Union[List[float], np.ndarray, pd.Series]\n",
      " |          Input ground truth for training (targets)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      None\n",
      " |  \n",
      " |  get_best_params(self) -> Dict[str, Union[str, float, int]]\n",
      " |      Returns the tuned results of the optimization as the best set of hyper-parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Dict[str, Union[str, float, int]]\n",
      " |  \n",
      " |  get_params_bounds(self) -> Optional[Dict[str, Any]]\n",
      " |      Returns the hyper-parameters boundaries for the tuning process.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Dict[str, Any]\n",
      " |  \n",
      " |  get_results(self) -> List[Dict[str, Any]]\n",
      " |      Return all trials results.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      List[Dict[str, Any]]\n",
      " |  \n",
      " |  get_trials(self) -> hyperopt.base.Trials\n",
      " |      Returns the `Trials` object passed to the optimizer.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      hyperopt.Trials\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'early_stopping_rounds': typing.Optional[int], 'imp...\n",
      " |  \n",
      " |  __dataclass_fields__ = {'early_stopping_rounds': Field(name='early_sto...\n",
      " |  \n",
      " |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  early_stopping_rounds = 20\n",
      " |  \n",
      " |  importance_type = 'total_gain'\n",
      " |  \n",
      " |  metrics = 'auc'\n",
      " |  \n",
      " |  n_iter = 100\n",
      " |  \n",
      " |  n_splits = 4\n",
      " |  \n",
      " |  num_boost_round = 200\n",
      " |  \n",
      " |  objective = 'binary:logistic'\n",
      " |  \n",
      " |  params_bounds = None\n",
      " |  \n",
      " |  random_state = 1367\n",
      " |  \n",
      " |  scale_mean = False\n",
      " |  \n",
      " |  scale_std = False\n",
      " |  \n",
      " |  shuffle = True\n",
      " |  \n",
      " |  sparse_matrix = False\n",
      " |  \n",
      " |  stratified = True\n",
      " |  \n",
      " |  verbose = True\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from slickml.base._estimator.BaseXGBoostEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from slickml.base._estimator.BaseXGBoostEstimator:\n",
      " |  \n",
      " |  params = None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Example 1: `XGBoostHyperOptimizer` for `classification` using `breast-cancer` data set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "clf_data = load_breast_cancer()\n",
    "X_clf, y_clf = clf_data.data, clf_data.target"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "xho_clf = XGBoostHyperOptimizer(\n",
    "    n_iter=10,\n",
    "    metrics=\"auc\",\n",
    "    objective=\"binary:logistic\",\n",
    ")\n",
    "xho_clf.fit(X_clf, y_clf)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 10.37trial/s, best loss: -0.9942836784208419]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# tuned parameters (or xho_clf.get_best_params())\n",
    "xho_clf.best_params_"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.48,\n",
       " 'gamma': 0.44,\n",
       " 'learning_rate': 0.53,\n",
       " 'max_depth': 2,\n",
       " 'min_child_weight': 2.0,\n",
       " 'reg_alpha': 0.05,\n",
       " 'reg_lambda': 0.63,\n",
       " 'subsample': 0.98}"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# array of all trials (or xho_clf.get_results())\n",
    "# indexing only the last member for the sake of print\n",
    "xho_clf.results_[-1]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'state': 2,\n",
       " 'tid': 9,\n",
       " 'spec': None,\n",
       " 'result': {'loss': -0.5, 'status': 'ok'},\n",
       " 'misc': {'tid': 9,\n",
       "  'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "  'workdir': None,\n",
       "  'idxs': {'colsample_bytree': [9],\n",
       "   'gamma': [9],\n",
       "   'learning_rate': [9],\n",
       "   'max_depth': [9],\n",
       "   'min_child_weight': [9],\n",
       "   'reg_alpha': [9],\n",
       "   'reg_lambda': [9],\n",
       "   'subsample': [9]},\n",
       "  'vals': {'colsample_bytree': [0.81],\n",
       "   'gamma': [0.88],\n",
       "   'learning_rate': [0.43],\n",
       "   'max_depth': [1],\n",
       "   'min_child_weight': [13.0],\n",
       "   'reg_alpha': [0.42],\n",
       "   'reg_lambda': [0.8300000000000001],\n",
       "   'subsample': [0.18]}},\n",
       " 'exp_key': None,\n",
       " 'owner': None,\n",
       " 'version': 0,\n",
       " 'book_time': datetime.datetime(2022, 11, 28, 4, 27, 20, 697000),\n",
       " 'refresh_time': datetime.datetime(2022, 11, 28, 4, 27, 20, 751000)}"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# trial object (or xho_clf.get_trials())\n",
    "xho_clf.trials_"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<hyperopt.base.Trials at 0x137f16d30>"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# parameter bounds\n",
    "xho_clf.get_params_bounds()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'max_depth': <hyperopt.pyll.base.Apply at 0x137f24400>,\n",
       " 'learning_rate': <hyperopt.pyll.base.Apply at 0x137f24640>,\n",
       " 'min_child_weight': <hyperopt.pyll.base.Apply at 0x137f24790>,\n",
       " 'colsample_bytree': <hyperopt.pyll.base.Apply at 0x137f248e0>,\n",
       " 'subsample': <hyperopt.pyll.base.Apply at 0x137f24a30>,\n",
       " 'gamma': <hyperopt.pyll.base.Apply at 0x137f24b80>,\n",
       " 'reg_alpha': <hyperopt.pyll.base.Apply at 0x137f24cd0>,\n",
       " 'reg_lambda': <hyperopt.pyll.base.Apply at 0x137f24e20>}"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Example 2: `XGBoostHyperOptimizer` for `regression` using `california-housing` data set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "reg_data = fetch_california_housing()\n",
    "X_reg, y_reg = reg_data.data, reg_data.target"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "xho_reg = XGBoostHyperOptimizer(\n",
    "    n_iter=10,\n",
    "    metrics=\"rmse\",\n",
    "    objective=\"reg:squarederror\",\n",
    ")\n",
    "xho_reg.fit(X_reg, y_reg)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "100%|██████████| 10/10 [00:07<00:00,  1.39trial/s, best loss: 0.4888611586512855]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# hyper-parameter optimization results (or xho_reg.results_)\n",
    "xho_reg.get_results()[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'state': 2,\n",
       " 'tid': 0,\n",
       " 'spec': None,\n",
       " 'result': {'loss': 0.5048283668706345, 'status': 'ok'},\n",
       " 'misc': {'tid': 0,\n",
       "  'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "  'workdir': None,\n",
       "  'idxs': {'colsample_bytree': [0],\n",
       "   'gamma': [0],\n",
       "   'learning_rate': [0],\n",
       "   'max_depth': [0],\n",
       "   'min_child_weight': [0],\n",
       "   'reg_alpha': [0],\n",
       "   'reg_lambda': [0],\n",
       "   'subsample': [0]},\n",
       "  'vals': {'colsample_bytree': [0.68],\n",
       "   'gamma': [0.05],\n",
       "   'learning_rate': [0.84],\n",
       "   'max_depth': [0],\n",
       "   'min_child_weight': [12.0],\n",
       "   'reg_alpha': [0.8200000000000001],\n",
       "   'reg_lambda': [0.5],\n",
       "   'subsample': [0.87]}},\n",
       " 'exp_key': None,\n",
       " 'owner': None,\n",
       " 'version': 0,\n",
       " 'book_time': datetime.datetime(2022, 11, 28, 4, 27, 21, 64000),\n",
       " 'refresh_time': datetime.datetime(2022, 11, 28, 4, 27, 21, 963000)}"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# best results (or xbo_ref.best_params_)\n",
    "xho_reg.get_best_params()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.89,\n",
       " 'gamma': 0.48,\n",
       " 'learning_rate': 0.24,\n",
       " 'max_depth': 2,\n",
       " 'min_child_weight': 3.0,\n",
       " 'reg_alpha': 0.44,\n",
       " 'reg_lambda': 0.8,\n",
       " 'subsample': 0.39}"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feel free to add your favorite `Example` via a `pull-request`.\n",
    "### More details can be found in our [Contributing Document](https://github.com/slickml/slick-ml/blob/master/CONTRIBUTING.md)."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.12 64-bit ('.venv': poetry)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "interpreter": {
   "hash": "07bad374b921cdabaf2ef6a1d4ae5a7996d892e7452f8d9d13efced363d002df"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}